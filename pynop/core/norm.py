from functools import partial

import torch
from torch import nn, Tensor
import torch.nn.functional as F

""" Note sur LayerNorm vs InstanceNorm:
- LayerNorm: gamma aet beta sont de dimension normalized_shape
- InstanceNorm: gamma etd beta sont de dimesnions C (1 paramètre par canal)
"""


class LayerNorm2d(nn.LayerNorm):
    """LayerNorm telle que definie dans ConvNext: moyenne et std calculé le long des canaux"""

    # normalized_shape MUST be defined when intanciating the Layer
    def forward(self, x: Tensor) -> Tensor:
        x = x.permute(0, 2, 3, 1)
        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        x = x.permute(0, 3, 1, 2)
        return x


class TimeConditionedLayerNorm(nn.Module):
    """Conditional LayerNorm, to be used with time conditioning for example.
    inspired from https://github.com/camlab-ethz/poseidon/blob/main/scOT/model.py#L135"""

    def __init__(self, dim, eps=1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Linear(1, dim)
        self.bias = nn.Linear(1, dim)

    def forward(self, x, time):
        mean = x.mean(dim=1, keepdim=True)
        var = (x**2).mean(dim=1, keepdim=True) - mean**2
        x = (x - mean) / (var + self.eps).sqrt()
        weight = self.weight(time)
        bias = self.bias(time)
        dims_to_unsqueeze = x.dim() - weight.dim()
        for _ in range(dims_to_unsqueeze):
            weight = weight.unsqueeze(-1)
            bias = bias.unsqueeze(-1)
        return weight * x + bias


class TimeConditionedLayerNormv2(nn.Module):
    """
    Conditional LayerNorm in which the parameters are generated by a MLP using the time embedding as input.
    """

    def __init__(self, dim, time_dim, hidden_dim=64, eps=1e-5):
        super().__init__()
        self.eps = eps
        self.generator = nn.Sequential(
            nn.Linear(time_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, dim * 2),  # Sortie de dimension 2 * C
        )
        self.num_channels = dim

    def forward(self, x, time):
        """x : Tensor of shape (B, C, ...)
        time: Tensor of shape (B, time_dim)
        """
        mean = x.mean(dim=1, keepdim=True)
        var = (x**2).mean(dim=1, keepdim=True) - mean**2
        x = (x - mean) / (var + self.eps).sqrt()
        params = self.generator(time)
        weight, bias = torch.split(params, self.num_channels, dim=-1)
        dims_to_unsqueeze = x.dim() - weight.dim()
        for _ in range(dims_to_unsqueeze):
            weight = weight.unsqueeze(-1)
            bias = bias.unsqueeze(-1)
        return weight * x + bias


class RMSNorm2d(nn.RMSNorm):
    # Normalize each channel separately (normalized_shape MUST be defined when intanciating the Layer)
    def forward(self, x: Tensor) -> Tensor:
        x = x.permute(0, 2, 3, 1)
        x = F.rms_norm(x, self.normalized_shape, self.weight, self.eps)
        x = x.permute(0, 3, 1, 2)
        return x


NORM_DICT = {"ln": LayerNorm2d, "gn": nn.GroupNorm, "bn": nn.BatchNorm2d, "rmsn": RMSNorm2d}


def build_norm(norm: str, num_features: int = None, **kwargs):

    if norm is None:
        return None
    if norm.lower() in NORM_DICT:
        if norm == "gn":
            try:
                groups = kwargs.pop("groups")
            except KeyError:
                groups = 32
            return nn.GroupNorm(num_groups=groups, num_channels=num_features, **kwargs)
        elif norm == "bn":
            return nn.BatchNorm2d(num_features=num_features, **kwargs)
        elif norm in ["ln", "rmsn"]:
            return NORM_DICT[norm](normalized_shape=num_features, **kwargs)
    else:
        return None
